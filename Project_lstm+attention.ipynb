{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tz7tpDd9_iTp"
   },
   "source": [
    "## Packages and constant declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uVbTNVII_iTw",
    "outputId": "9e4e17b7-8ab0-4135-a3df-cdf788b8845c"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tensorboardX import SummaryWriter\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2CxEa0Yy_iT2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62Jg-3ql_iT-"
   },
   "outputs": [],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # easy to locate traceback\n",
    "EMBEDDING_DIM = 300 # embedding vector length\n",
    "max_word = 200000 # How many unique words to use \n",
    "batch_size = 1024#Batch size\n",
    "device = torch.device('cuda')\n",
    "random_seed=123\n",
    "BIDIRECTIONAL= True\n",
    "HIDDEN_DIM= 32\n",
    "NUM_LAYERS=2\n",
    "OUTPUT_DIM=1\n",
    "DROPOUT=0.5\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErIO6kZ6_iUB"
   },
   "outputs": [],
   "source": [
    "def seed(seed=1000):# Give random seed to everything\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n6zdldK5_iUD"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEKwrU5N_iUE"
   },
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean(x): #process the punction\n",
    "    x=str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x=x.replace(punct,f'{punct}')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x): # Replace the number with #, make all numbers the same\n",
    "    if bool(re.search(r'\\d',x)):\n",
    "        x=re.sub('[0-9]{5,}','#####',x)\n",
    "        x=re.sub('[0-9]{4}','####',x)\n",
    "        x=re.sub('[0-9]{3}','###',x)\n",
    "        x=re.sub('[0-9]{2}','##',x)\n",
    "    return x\n",
    "\n",
    "# The common abbreviation for some phrase\n",
    "fullversiondict={\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\",\n",
    "                 \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n",
    "                 \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                 \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
    "                 \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\",\n",
    "                 \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                 \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                 \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                 \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                 \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                 \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \n",
    "                 \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n",
    "                 \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                 \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
    "                 \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
    "                 \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
    "                 \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
    "                 \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', \n",
    "                 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do',\n",
    "                 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018',\n",
    "                 'qouta': 'quota','exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "def _get_full(fullversiondict):\n",
    "    full_re = re.compile('(%s)'%'|'.join(fullversiondict.keys()))\n",
    "    return fullversiondict , full_re # To make things we want to replace single string\n",
    "\n",
    "fullversiondict,full_re=_get_full(fullversiondict)\n",
    "def replacetext(text):\n",
    "    def replace(match): \n",
    "        return fullversiondict[match.group(0)]\n",
    "    return full_re.sub(replace,text) # The full.re here is match, can replace any string now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1YmE8yu_iUI"
   },
   "outputs": [],
   "source": [
    "def read_pre():\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    print('Train shape:',train_df.shape)\n",
    "    print('Test shape:',test_df.shape)\n",
    "    # Lower case\n",
    "    train_df['question_text'] = train_df['question_text'].apply(lambda x: x.lower())\n",
    "    test_df['question_text'] = test_df['question_text'].apply(lambda x: x.lower())\n",
    "    # Clean punction\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean(x))\n",
    "    #clean numbers\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "    #replace abbreviation\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: replacetext(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: replacetext(x))\n",
    "    ## fill up the missing values using next valid value\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].fillna(method=\"bfill\")\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].fillna(method=\"bfill\")\n",
    "    \n",
    "#     #Target\n",
    "#     train_y = train_df['target']\n",
    "    train_df.to_csv(\"train_df.csv\")\n",
    "    test_df.to_csv(\"test_df.csv\")\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9oBE4kV_iUN"
   },
   "outputs": [],
   "source": [
    "# the pretrained embedding model\n",
    "def load_glove(word_index):\n",
    "    FILE= 'embeddings/glove.840B.300d//glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr):return word, np.asarray(arr, dtype='float32')[:EMBEDDING_DIM]\n",
    "    embeddings_index = dict(get_coefs(*o.split(' '))for o in open(FILE) if len(o)>300)\n",
    "    #construct word array pair dictionary\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    nb_words = min(max_word, len(word_index))\n",
    "    #Initialize a matrix using random value, in case that some words don't exist in our embedding\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words+1,EMBEDDING_DIM))\n",
    "    #Associate the word in our training set with the embedding model\n",
    "    for word, i in word_index.items():\n",
    "        if i>= max_word: continue\n",
    "        embedding_vector = embeddings_index.get(word) # get the vector for this word form dictionary\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    return (embedding_matrix)\n",
    "##Do same thing to other types of embedding\n",
    "\n",
    "def load_para(word_index):\n",
    "    FILE = 'embeddings/paragram_300_sl999-1/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(FILE, encoding=\"utf8\", errors='ignore') if len(o)>300)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "    nb_words = min(max_word, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words+1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_word: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "xnjBAGru_iUQ",
    "outputId": "969d6477-3c6e-468b-d185-52ac4ea4b3d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1306122, 3)\n",
      "Test shape: (375806, 2)\n"
     ]
    }
   ],
   "source": [
    "read_pre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.read_csv(\"train_df.csv\")\n",
    "test_X = pd.read_csv(\"test_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train_X[['question_text','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.16287588169781"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[train_X.target == 0].shape[0]/train_X[train_X.target == 1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_X\n",
    "del test_X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5mLCeCvRJ5yO",
    "outputId": "34acff9e-3e0a-47f8-f080-744752bb10e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 1404820\n",
      "Num Valid: 351205\n"
     ]
    }
   ],
   "source": [
    "train, valid = train_test_split(train,test_size=0.2, random_state=random_seed, stratify = train.target,shuffle=True)\n",
    "print(f'Num Train: {len(train)}')\n",
    "print(f'Num Valid: {len(valid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.263572990141484"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oversample\n",
    "attach = train[train.target == 1]\n",
    "for i in range(11):\n",
    "    train=pd.concat((train,attach))\n",
    "train[train.target == 0].shape[0]/train[train.target == 1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>227664</td>\n",
       "      <td>how did the mtv network's audience respond to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334082</td>\n",
       "      <td>will there be censorship in duckduckgo image s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1072158</td>\n",
       "      <td>why do the majority of people on quora seem to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323157</td>\n",
       "      <td>who invented zero (0) and one (1)?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>622521</td>\n",
       "      <td>how does the arabic culture differ from the ir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question_text  target\n",
       "227664   how did the mtv network's audience respond to ...       0\n",
       "334082   will there be censorship in duckduckgo image s...       0\n",
       "1072158  why do the majority of people on quora seem to...       0\n",
       "323157                  who invented zero (0) and one (1)?       0\n",
       "622521   how does the arabic culture differ from the ir...       0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy', sequential=True, include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "def get_dataset(csv_data, text_field, label_field):\n",
    "    fields = [(\"question_text\", text_field), (\"target\", label_field)]       \n",
    "    examples = []\n",
    "    for text, label in zip(csv_data['question_text'], csv_data['target']):\n",
    "            examples.append(data.Example.fromlist([text, label], fields))\n",
    "    return examples, fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples, train_fields = get_dataset(train, TEXT, LABEL)\n",
    "valid_examples, valid_fields = get_dataset(valid, TEXT, LABEL)\n",
    "train = data.Dataset(train_examples, train_fields)\n",
    "valid = data.Dataset(valid_examples, valid_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator= data.BucketIterator.splits(\n",
    "    (train, valid), \n",
    "    batch_size = batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.question_text),\n",
    "    device = device,\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only run at the first time\n",
    "cache = '.vector_cache'\n",
    "if not os.path.exists(cache):\n",
    "    os.mkdir(cache)\n",
    "vectors = torchtext.vocab.Vectors(name='.vector_cache/glove.840B.300d.txt', cache=cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 162910\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train.question_text,\n",
    "                 max_size=max_word,\n",
    "                 vectors=vectors,\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train.target)\n",
    "\n",
    "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
    "print(f'Number of classes: {len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train \n",
    "del valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text matrix size: torch.Size([34, 5000])\n",
      "Target vector size: torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    print(f'Text matrix size: {batch.question_text[0].size()}')\n",
    "    print(f'Target vector size: {batch.target.size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AMT6P7uohfz"
   },
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKQrTfMT_iUL"
   },
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), (K, 1) are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "#Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(data_loader):\n",
    "            text, text_lengths = batch_data.question_text\n",
    "            logits = model(text, text_lengths)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "            num_examples += batch_data.target.size(0)\n",
    "            correct_pred += (predicted_labels.long() == batch_data.target.long()).sum()\n",
    "        return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UejUcWj5bIsA"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = (BIDIRECTIONAL+1) * HIDDEN_DIM\n",
    "        self.num_layers = 2\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), EMBEDDING_DIM)\n",
    "        self.embedding.weight.data.copy_(weight_matrix )\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        #The embedding vectors here are not parameter needing update\n",
    "        self.embedding_dropout = SpatialDropout(0.15)\n",
    "        #  (Tensor data, Tensor batch_sizes, tuple of Tensors hx, tuple of Tensors params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional)\n",
    "        self.rnn = nn.LSTM(input_size = EMBEDDING_DIM, \n",
    "                           hidden_size = HIDDEN_DIM,\n",
    "                           num_layers = self.num_layers,\n",
    "                           bidirectional = BIDIRECTIONAL, \n",
    "                           dropout = 0.5)\n",
    "        self.fc1 = nn.Linear((BIDIRECTIONAL+1) * HIDDEN_DIM, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.spatialdrop = SpatialDropout(0.5)\n",
    "        \n",
    "        self.weight_W = nn.Parameter(torch.Tensor(self.hidden_dim, self.hidden_dim))\n",
    "        self.weight_proj = nn.Parameter(torch.Tensor(self.hidden_dim, 1))\n",
    "        nn.init.uniform_(self.weight_W, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.weight_proj, -0.1, 0.1)\n",
    "        \n",
    "    def forward(self, text, text_length):\n",
    "\n",
    "        embedded = self.spatialdrop(self.embedding(text))\n",
    "        packed_output, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        u = torch.tanh(torch.matmul(packed_output, self.weight_W))\n",
    "        att = torch.matmul(u, self.weight_proj)\n",
    "        att_score = F.softmax(att, dim=0)\n",
    "        scored_x = packed_output * att_score\n",
    "        \n",
    "        hidden = torch.sum(scored_x, dim=0)\n",
    "        hidden = self.fc1(hidden)\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = self.fc2(hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "model = RNN()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/005 | Batch 000/1372 | Cost: 0.2410\n",
      "Epoch: 001/005 | Batch 100/1372 | Cost: 0.2398\n",
      "Epoch: 001/005 | Batch 200/1372 | Cost: 0.2149\n",
      "Epoch: 001/005 | Batch 300/1372 | Cost: 0.1825\n",
      "Epoch: 001/005 | Batch 400/1372 | Cost: 0.2406\n",
      "Epoch: 001/005 | Batch 500/1372 | Cost: 0.2176\n",
      "Epoch: 001/005 | Batch 600/1372 | Cost: 0.1955\n",
      "Epoch: 001/005 | Batch 700/1372 | Cost: 0.2203\n",
      "Epoch: 001/005 | Batch 800/1372 | Cost: 0.2477\n",
      "Epoch: 001/005 | Batch 900/1372 | Cost: 0.2283\n",
      "Epoch: 001/005 | Batch 1000/1372 | Cost: 0.2150\n",
      "Epoch: 001/005 | Batch 1100/1372 | Cost: 0.2134\n",
      "Epoch: 001/005 | Batch 1200/1372 | Cost: 0.2368\n",
      "Epoch: 001/005 | Batch 1300/1372 | Cost: 0.2238\n",
      "training accuracy: 55.82%\n",
      "valid accuracy: 55.82%\n",
      "Time elapsed: 1.57 min\n",
      "Epoch: 002/005 | Batch 000/1372 | Cost: 0.1770\n",
      "Epoch: 002/005 | Batch 100/1372 | Cost: 0.1956\n",
      "Epoch: 002/005 | Batch 200/1372 | Cost: 0.2257\n",
      "Epoch: 002/005 | Batch 300/1372 | Cost: 0.1817\n",
      "Epoch: 002/005 | Batch 400/1372 | Cost: 0.2313\n",
      "Epoch: 002/005 | Batch 500/1372 | Cost: 0.1835\n",
      "Epoch: 002/005 | Batch 600/1372 | Cost: 0.2243\n",
      "Epoch: 002/005 | Batch 700/1372 | Cost: 0.1922\n",
      "Epoch: 002/005 | Batch 800/1372 | Cost: 0.2826\n",
      "Epoch: 002/005 | Batch 900/1372 | Cost: 0.2394\n",
      "Epoch: 002/005 | Batch 1000/1372 | Cost: 0.2455\n",
      "Epoch: 002/005 | Batch 1100/1372 | Cost: 0.2229\n",
      "Epoch: 002/005 | Batch 1200/1372 | Cost: 0.1889\n",
      "Epoch: 002/005 | Batch 1300/1372 | Cost: 0.2332\n",
      "training accuracy: 55.82%\n",
      "valid accuracy: 55.82%\n",
      "Time elapsed: 3.06 min\n",
      "Epoch: 003/005 | Batch 000/1372 | Cost: 0.1713\n",
      "Epoch: 003/005 | Batch 100/1372 | Cost: 0.2088\n",
      "Epoch: 003/005 | Batch 200/1372 | Cost: 0.2009\n",
      "Epoch: 003/005 | Batch 300/1372 | Cost: 0.2414\n",
      "Epoch: 003/005 | Batch 400/1372 | Cost: 0.2105\n",
      "Epoch: 003/005 | Batch 500/1372 | Cost: 0.1723\n",
      "Epoch: 003/005 | Batch 600/1372 | Cost: 0.1730\n",
      "Epoch: 003/005 | Batch 700/1372 | Cost: 0.2596\n",
      "Epoch: 003/005 | Batch 800/1372 | Cost: 0.2105\n",
      "Epoch: 003/005 | Batch 900/1372 | Cost: 0.2142\n",
      "Epoch: 003/005 | Batch 1000/1372 | Cost: 0.2229\n",
      "Epoch: 003/005 | Batch 1100/1372 | Cost: 0.1942\n",
      "Epoch: 003/005 | Batch 1200/1372 | Cost: 0.1705\n",
      "Epoch: 003/005 | Batch 1300/1372 | Cost: 0.2302\n",
      "training accuracy: 55.82%\n",
      "valid accuracy: 55.82%\n",
      "Time elapsed: 4.59 min\n",
      "Epoch: 004/005 | Batch 000/1372 | Cost: 0.1870\n",
      "Epoch: 004/005 | Batch 100/1372 | Cost: 0.2187\n",
      "Epoch: 004/005 | Batch 200/1372 | Cost: 0.2050\n",
      "Epoch: 004/005 | Batch 300/1372 | Cost: 0.2059\n",
      "Epoch: 004/005 | Batch 400/1372 | Cost: 0.2374\n",
      "Epoch: 004/005 | Batch 500/1372 | Cost: 0.2421\n",
      "Epoch: 004/005 | Batch 600/1372 | Cost: 0.2081\n",
      "Epoch: 004/005 | Batch 700/1372 | Cost: 0.2384\n",
      "Epoch: 004/005 | Batch 800/1372 | Cost: 0.2269\n",
      "Epoch: 004/005 | Batch 900/1372 | Cost: 0.1949\n",
      "Epoch: 004/005 | Batch 1000/1372 | Cost: 0.2206\n",
      "Epoch: 004/005 | Batch 1100/1372 | Cost: 0.2142\n",
      "Epoch: 004/005 | Batch 1200/1372 | Cost: 0.1925\n",
      "Epoch: 004/005 | Batch 1300/1372 | Cost: 0.1811\n",
      "training accuracy: 55.82%\n",
      "valid accuracy: 55.82%\n",
      "Time elapsed: 6.13 min\n",
      "Epoch: 005/005 | Batch 000/1372 | Cost: 0.2011\n",
      "Epoch: 005/005 | Batch 100/1372 | Cost: 0.1862\n",
      "Epoch: 005/005 | Batch 200/1372 | Cost: 0.1853\n",
      "Epoch: 005/005 | Batch 300/1372 | Cost: 0.2288\n",
      "Epoch: 005/005 | Batch 400/1372 | Cost: 0.2219\n",
      "Epoch: 005/005 | Batch 500/1372 | Cost: 0.2025\n",
      "Epoch: 005/005 | Batch 600/1372 | Cost: 0.1868\n",
      "Epoch: 005/005 | Batch 700/1372 | Cost: 0.2193\n",
      "Epoch: 005/005 | Batch 800/1372 | Cost: 0.1845\n",
      "Epoch: 005/005 | Batch 900/1372 | Cost: 0.2234\n",
      "Epoch: 005/005 | Batch 1000/1372 | Cost: 0.1794\n",
      "Epoch: 005/005 | Batch 1100/1372 | Cost: 0.1570\n",
      "Epoch: 005/005 | Batch 1200/1372 | Cost: 0.1811\n",
      "Epoch: 005/005 | Batch 1300/1372 | Cost: 0.2079\n",
      "training accuracy: 55.82%\n",
      "valid accuracy: 55.82%\n",
      "Time elapsed: 7.64 min\n",
      "Epoch: 006/005 | Batch 000/1372 | Cost: 0.2112\n",
      "Epoch: 006/005 | Batch 100/1372 | Cost: 0.1905\n",
      "Epoch: 006/005 | Batch 200/1372 | Cost: 0.2486\n",
      "Epoch: 006/005 | Batch 300/1372 | Cost: 0.2455\n",
      "Epoch: 006/005 | Batch 400/1372 | Cost: 0.2370\n",
      "Epoch: 006/005 | Batch 500/1372 | Cost: 0.1626\n",
      "Epoch: 006/005 | Batch 600/1372 | Cost: 0.2259\n",
      "Epoch: 006/005 | Batch 700/1372 | Cost: 0.2026\n",
      "Epoch: 006/005 | Batch 800/1372 | Cost: 0.2268\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-127aefda201a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquestion_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\data\\iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\data\\batch.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, dataset, device)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                     \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, batch, device)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \"\"\"\n\u001b[0;32m    236\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[1;34m(self, arr, device)\u001b[0m\n\u001b[0;32m    357\u001b[0m                 \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m         \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_iterator):\n",
    "        \n",
    "        text, text_lengths = batch_data.question_text\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(text, text_lengths)\n",
    "#         cost = F.cross_entropy(logits, batch_data.target)\n",
    "        cost=loss_fn(torch.squeeze(logits),batch_data.target)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 100:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_iterator):03d} | '\n",
    "                   f'Cost: {cost:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_iterator):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_iterator):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SdJ5GlwBYqIq"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project_cnn_2.0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
