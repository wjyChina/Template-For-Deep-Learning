{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FFNN_binary_classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjyChina/Template-For-Deep-Learning/blob/master/FFNN_binary_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amMgR5bHLo9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import numpy as np  #linear algebra\n",
        "import pandas as pd #Only CSV IO\n",
        "import os\n",
        "import re\n",
        "from torch.utils import data #dataloader of batch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from torch.autograd import Variable #We can ask require grad or not\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import gc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL6tCm-zJtj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Mount Google drive to gain access to data in it ###\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrOwZHzCLrIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Change here ###\n",
        "### Mount the folder of your data. You can use !ls to check all the folders in your current path ###\n",
        "os.chdir(\"drive/My Drive/Colab Notebooks/\") \n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MkPKS3rMEcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from keras.utils import np_utils\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RpzMwlLMP7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Change here ###\n",
        "### Read in your data ###\n",
        "data=pd.read_csv('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH4EBfvXMhEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Change here ###\n",
        "### Split your data by data_y. First you need to split the original data into input and target ###\n",
        "data_x=\n",
        "data_y=\n",
        "x_train,x_test,y_train,y_test = train_test_split(data_x,data_y,test_size=0.25,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i4gBgLlNBVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train=np.array(x_train)\n",
        "y_train=np.array(y_train)\n",
        "x_test=np.array(x_test)\n",
        "y_test=np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwdKm7k6NG8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Change here ###\n",
        "### Hyperparameters for training ###\n",
        "seed = 1\n",
        "learning_rate = 0.001\n",
        "batch_size = 1024\n",
        "n_epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx2Ga9zqNh5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Change here ###\n",
        "class DP(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MLP, self).__init__()\n",
        "          \n",
        "    self.linear_1 = nn.Linear(722, 526)\n",
        "    self.linear_2 = nn.Linear(526, 258)\n",
        "    self.linear_3 = nn.Linear(258,56)\n",
        "    self.linear_out = nn.Linear(56, 1)\n",
        "        \n",
        "        \n",
        "  def forward(self, x):\n",
        "    out = self.linear_1(x)\n",
        "    # print(out)\n",
        "    out = torch.tanh(out)\n",
        "    out = F.dropout(out,p=0.5,training=self.training)\n",
        "    out = self.linear_2(out)\n",
        "    out = torch.tanh(out) \n",
        "    out = F.dropout(out,p=0.5,training=self.training)\n",
        "    out = self.linear_3(out)\n",
        "    out = torch.sigmoid(out) \n",
        "    out = F.dropout(out,p=0.5,training=self.training)     \n",
        "    logits = self.linear_out(out)\n",
        "    # print(logits)\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l86qwVSZNv9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Change here if you are going to use loss function other than BCEWithLogitsLoss ###\n",
        "def compute_epoch_loss(model, data_loader):\n",
        "  curr_loss, num_examples = 0., 0\n",
        "  loss_fn=nn.BCEWithLogitsLoss()\n",
        "  with torch.no_grad():\n",
        "    for features, targets in data_loader:\n",
        "      features = features.view(-1, 722).to(device)   ### Change here for different features number: 722 ###\n",
        "      targets = targets.to(device)\n",
        "      logits = model.forward(features)\n",
        "      loss = loss_fn(torch.squeeze(logits),targets)\n",
        "      num_examples += targets.size()[0]\n",
        "      curr_loss += loss*targets.size()[0]\n",
        "\n",
        "    curr_loss = curr_loss / num_examples\n",
        "    return curr_loss\n",
        "\n",
        "    \n",
        "def compute_accuracy(model, data_loader):\n",
        "  correct_pred, num_examples = 0, 0\n",
        "  l=0\n",
        "  with torch.no_grad():\n",
        "    for features, targets in data_loader:\n",
        "      features = features.view(-1, 722).to(device)   ### Change here for different features number: 722 ###\n",
        "      targets = targets.to(device)\n",
        "      targets = torch.squeeze(targets)\n",
        "      logits = model.forward(features)\n",
        "      predicted_labels = torch.sigmoid(logits)\n",
        "      num_examples += targets.size(0)\n",
        "      predicted_labels = torch.squeeze(predicted_labels)\n",
        "      correct_pred += (((np.array(predicted_labels.cpu())>0.5).astype(int) == np.array(targets.cpu())).sum())\n",
        "    # print(correct_pred)\n",
        "    return correct_pred/num_examples * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvys94Z9OA7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainfoldx = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
        "trainfoldy = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "x_val_fold = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
        "y_val_fold = torch.tensor(y_test[:,np.newaxis], dtype=torch.float32).to(device)\n",
        "\n",
        "train = torch.utils.data.TensorDataset(trainfoldx,trainfoldy)\n",
        "valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train , batch_size=batch_size,shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y94ITj8ROEsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Train ###\n",
        "mbatch_cost = []\n",
        "start_time = time.time()\n",
        "minibatch_cost = []\n",
        "torch.manual_seed(seed)\n",
        "model = DP()\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001, weight_decay=1e-8)  ### Change here for different optimizer ###\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()                     ### Change here for different loss function ###\n",
        "for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    model.train()\n",
        "    grand_loss = 0\n",
        "    for  batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
        "        logits = model(x_batch)\n",
        "        loss=loss_fn(torch.squeeze(logits),y_batch)\n",
        "\n",
        "        #backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        mbatch_cost.append(loss.item())\n",
        "        if not batch_idx % 50:                    ### Change the number '50' if you want a different interval of output ###\n",
        "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n",
        "                %(epoch+1, n_epochs, batch_idx, len(train_loader), loss.item()))\n",
        "\n",
        "                  #evaluation          \n",
        "    model.eval()\n",
        "    cost = compute_epoch_loss(model, train_loader)\n",
        "    # cost_test = compute_epoch_loss(model, valid_loader)\n",
        "    # epoch_cost.append(cost)\n",
        "\n",
        "    train_accuracy = compute_accuracy(model, train_loader)\n",
        "    valid_accuracy = compute_accuracy(model, valid_loader)\n",
        "\n",
        "    print('Epoch: %03d/%03d Train Cost: %.4f' % (\n",
        "            epoch+1, n_epochs, cost))\n",
        "    print('Train Accuracy: %.3f | Test Accuracy: %.3f' % (train_accuracy, valid_accuracy))\n",
        "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
        "\n",
        "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zWbdX2TOg6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Plot minibatch v.s. loss ###\n",
        "plt.plot(mbatch_cost)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhL7efXwOoSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Get prediction and model judgement metrics ###\n",
        "logits = model(x_val_fold)\n",
        "predicted_prob = torch.sigmoid(logits)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, np.array(predicted_prob.detach().cpu()))\n",
        "plt.plot(fpr, tpr, 'b')\n",
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('TP')\n",
        "plt.xlabel('FP')\n",
        "plt.show()\n",
        "AUC = metrics.auc(fpr, tpr)\n",
        "print('AUC=',AUC)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "pred=(np.array(predicted_prob.detach().cpu())>0.5).astype(int)  ### Change your shreshold here ###\n",
        "con=confusion_matrix(y_test,pred)\n",
        "accuracy=(con[0,0]+con[1,1])/(con[0,1]+con[1,0]+con[0,0]+con[1,1])\n",
        "precision=con[0,0]/(con[0,0]+con[1,0])\n",
        "recall=con[0,0]/(con[0,0]+con[0,1])\n",
        "print('accuracy=',accuracy)\n",
        "print('error=',1-accuracy)\n",
        "print('precision=',precision)\n",
        "print('recall=',recall)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}